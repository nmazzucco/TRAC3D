# This script has been made to classify a series of archaeological tools, both harvesting and potential threshing sledge inserts, by using an experimental reference collection.
# First of all, a classification model (using Discriminant Canonical Analysis) is run using the experimental data and successively the archaeological tools are blind-tested on it. Accurcy of the model us calculated using multinomial logistic regression.

# Load the required libraries
library(readxl)
library(openxlsx)
library(ggplot2)
library(dplyr)
library(writexl)
library(MASS)

# Specify the file path
file_path <- "C:\\Users\\nicco\\R\\TRAC3D\\THRESHING\\RAWDATA\\Threshing_all_def.xlsx"

# Read the Excel file
data <- read_excel(file_path)

# Split the data into training and test based on the values in column "CAT"
training_data <- data[data$CAT == 2, ]
test_data <- data[data$CAT == 1, ]

# Specify the file paths for the training and test files
training_file <- "C:\\Users\\nicco\\R\\TRAC3D\\THRESHING\\training.xlsx"
test_file <- "C:\\Users\\nicco\\R\\TRAC3D\\THRESHING\\test.xlsx"

# Write the training data to a new Excel file
write.xlsx(training_data, file = training_file, rownames = FALSE)

# Write the test data to a new Excel file
write.xlsx(test_data, file = test_file, rownames = FALSE)

# Specify the numerical predictors
numerical_predictors <- c("Sq",	"Ssk",	"Sku",	"Sp",	"Sv",	"Sz",	"Sa",	"Smrc",	"Smc",	"Sxp",	"Sal",	"Strs",	"Std",	"Sdq", "Sha", "Sds", "Sfd",	"Sdr",	"Vm",	"Vv",	"Vmp",	"Vmc",	"Vvc",	"Vvv",	"Spd",	"Spc",	"S10z",	"S5p",	"S5v",	"Sda",	"Sdv",	"Shv",	"Sk",	"Spk",	"Svk",	"Smr1",	"Sdar",	"Spar",	"St",	"SWt",	"Stdi",	"Ssc",	"Sdc",	"Sbi",	"Sci",	"Svi",	"Profundidadmax",	"Profundidadmed",	"Densidadmed")

# Subset the training data with the numerical predictors
selected_training_data <- training_data[, c("CODE_COD", numerical_predictors)]

# Remove rows with missing values
selected_training_data_clean <- na.omit(selected_training_data)
anyNA(selected_training_data$Densidadmed)

# Calculate the variances of each column
variances <- apply(selected_training_data_clean[, numerical_predictors], 2, var)
print(variances)

# Filter numerical predictors based on non-zero variance
non_zero_var_predictors <- numerical_predictors[variances > 0]
variances <- apply(selected_training_data_clean[, numerical_predictors], 2, var)
zero_variance_vars <- numerical_predictors[variances == 0]
numerical_predictors <- setdiff(numerical_predictors, zero_variance_vars)
print(zero_variance_vars)
selected_training_data_clean <- selected_training_data_clean[, !(colnames(selected_training_data_clean) %in% zero_variance_vars)]
numerical_predictors <- setdiff(numerical_predictors, zero_variance_vars)
non_zero_var_predictors <- setdiff(non_zero_var_predictors, zero_variance_vars)

# Remove rows with missing values
non_zero_var_predictors <- na.omit(non_zero_var_predictors)
numerical_predictors <- na.omit(numerical_predictors)
print(numerical_predictors)


###############################################################ELIMINATE PREDICTORS WITH LOW P-VALUES

# Calculate the correlation matrix
cor_matrix <- cor(selected_training_data_clean[, non_zero_var_predictors])

# Define the file path and name for the CSV file
file_path <- "C:/Users/nicco/R/TRAC3D/THRESHING/1_cor_matrix.csv"

# Save the cross table as a CSV file
write.csv(cor_matrix, file = file_path, row.names = TRUE)

# Create an empty vector to store p-values
p_values <- numeric(length(numerical_predictors))

# Iterate through each predictor
for (i in 1:length(numerical_predictors)) {
  # Perform correlation test and extract the p-value
  x <- as.numeric(selected_training_data_clean[[numerical_predictors[i]]])
  y <- as.numeric(selected_training_data_clean$CODE_COD)
  correlation_test <- cor.test(x, y)
  p_values[i] <- correlation_test$p.value
}

# Print or view the p-values
print(p_values)

# Create a data frame with predictors and p-values
p_values_df <- data.frame(Predictor = numerical_predictors, P_Value = p_values)

# Define the file path and name for the p-values CSV file
file_path_pvalues <- "C:/Users/nicco/R/TRAC3D/THRESHING/2_p_values.csv"

# Save the data frame as a CSV file
write.csv(p_values_df, file = file_path_pvalues, row.names = FALSE)

# Create an empty vector to store the predictors to remove
predictors_to_remove <- character(0)

# Iterate through each predictor
for (i in 1:length(p_values)) {
  # Check if the p-value is greater than 0.005
  if (p_values[i] > 0.005) {
    # Add the predictor name to the vector of predictors to remove
    predictors_to_remove <- c(predictors_to_remove, numerical_predictors[i])
  }
}

# Specify the outcome variable
outcome_var <- "CODE_COD"

# Create an empty data frame to store the results
univariate_result_df <- data.frame(Predictor = character(0), P_Value = numeric(0), stringsAsFactors = FALSE)

# Iterate through each predictor
for (predictor in colnames(selected_training_data_clean)[-1]) {
  # Perform t-test and extract the p-value
  p_value <- t.test(selected_training_data_clean[[predictor]], selected_training_data_clean[[outcome_var]])$p.value
  
  # Add the predictor and p-value to the data frame
  univariate_result_df <- rbind(univariate_result_df, data.frame(Predictor = predictor, P_Value = p_value, stringsAsFactors = FALSE))
}

# Sort the data frame by p-value in ascending order
univariate_result_df <- univariate_result_df[order(univariate_result_df$P_Value), ]

# Select the top 15 predictors with the lowest p-values
selected_predictors <- head(univariate_result_df$Predictor, 15)

# Print or view the selected predictors
print(selected_predictors)

# Save the selected predictors to a new CSV file
file_path <- "C:/Users/nicco/R/TRAC3D/THRESHING/3_selected_predictors.csv"
write.csv(selected_predictors, file = file_path, row.names = FALSE)

# Remove the predictors from selected_training_data_clean
selected_training_data_clean <- selected_training_data_clean[, !(names(selected_training_data_clean) %in% predictors_to_remove)]

# Extract the updated_predictors
updated_predictors <- tail(colnames(selected_training_data_clean)[-1], 11)

# Subset the data to include only the updated predictors
selected_training_data_clean_copy <- selected_training_data_clean[, c("CODE_COD", updated_predictors)]




###########################################################ELIMINATE CORRELATED PREDICTORS

# Calculate the correlation matrix
cor_matrix_updated_predictors <- cor(selected_training_data_clean_copy)

# Identify correlations greater than 0.8
high_correlations <- which(cor_matrix_updated_predictors > 0.8 & cor_matrix_updated_predictors < 1, arr.ind = TRUE)

# Print the pairs of highly correlated predictors
if (length(high_correlations) > 0) {
  for (i in 1:nrow(high_correlations)) {
    row_idx <- high_correlations[i, 1]
    col_idx <- high_correlations[i, 2]
    predictor1 <- rownames(cor_matrix_updated_predictors)[row_idx]
    predictor2 <- colnames(cor_matrix_updated_predictors)[col_idx]
    correlation <- cor_matrix_updated_predictors[row_idx, col_idx]
    cat("Predictor", predictor1, "and", predictor2, "are highly correlated with a correlation of", correlation, "\n")
  }
} else {
  cat("No predictors are highly correlated (correlation > 0.8).\n")
}

# Function to remove correlated predictors
remove_correlated_predictors <- function(data, cor_matrix, threshold = 0.8) {
  correlated_features <- which(cor_matrix > threshold & cor_matrix < 1, arr.ind = TRUE)
  
  removed_predictors <- character()
  for (i in 1:nrow(correlated_features)) {
    feature1 <- rownames(cor_matrix)[correlated_features[i, 1]]
    feature2 <- colnames(cor_matrix)[correlated_features[i, 2]]
    
    if (!(feature1 %in% removed_predictors) && !(feature2 %in% removed_predictors) && feature1 != feature2) {
      correlation <- cor_matrix[correlated_features[i, 1], correlated_features[i, 2]]
      cat(sprintf("Predictor %s and %s are highly correlated with a correlation of %.7f\n", feature1, feature2, correlation))
      
      pvalue1 <- anova(lm(data[[feature1]] ~ data[[feature2]]))$`Pr(>F)`[1]
      pvalue2 <- anova(lm(data[[feature2]] ~ data[[feature1]]))$`Pr(>F)`[1]
      
      if (pvalue1 < pvalue2) {
        data <- data[, !colnames(data) %in% feature1]
        removed_predictors <- c(removed_predictors, feature1)
      } else {
        data <- data[, !colnames(data) %in% feature2]
        removed_predictors <- c(removed_predictors, feature2)
      }
    }
  }
  
  return(list(data = data, removed_predictors = removed_predictors))
}


result <- remove_correlated_predictors(selected_training_data_clean_copy, cor_matrix_updated_predictors)
selected_training_data_clean <- result$data
removed_predictors <- result$removed_predictors

# Define the file path and name for the CSV file
file_path <- "C:/Users/nicco/R/TRAC3D/THRESHING/4_selected_training_data_clean.csv"

# Save the cross table as a CSV file
write.csv(selected_training_data_clean, file = file_path, row.names = TRUE)



 
#######################################################################################CANONICAL DISCRIMINANT ANALYSIS


# Perform Canonical Discriminant Analysis
cda_model <- lda(CODE_COD ~ ., data = selected_training_data_clean)

# Extract the discriminant scores for each observation
scores <- predict(cda_model)$x

# Create a data frame with scores and class labels
df <- data.frame(scores, Class = as.factor(selected_training_data_clean$CODE_COD))

# Check for missing values in scores
if (any(is.na(df$LDA1)) || any(is.na(df$LDA2))) {
  stop("Missing values detected in LD1 or LD2.")
}

# Check factor levels
if (!is.factor(df$Class)) {
  stop("Class variable (CODE_COD) should be a factor.")
}

# Remove missing values
df <- na.omit(df)

# Calculate centroids for each class
centroids <- aggregate(. ~ Class, data = df, FUN = mean)



#--------------------------------------------------STRUCTURE MATRIX

# Exclude CODE_COD and Prediction variables
variables <- selected_training_data_clean[, !(names(selected_training_data_clean) %in% c("CODE_COD", "Prediction"))]

# Convert variables to numeric format
variables <- as.data.frame(sapply(variables, as.numeric))

# Get the canonical function matrix
canonical_functions <- cda_model$scaling

# Transpose the canonical_functions matrix
canonical_functions_transposed <- t(canonical_functions)

# Get the coefficients for each variable in the canonical discriminant functions
coefficients <- canonical_functions_transposed

# Transpose the coefficients matrix for printing
coefficients_transposed <- t(coefficients)

# Print the ordered variables with their contributions and coefficients
print(coefficients_transposed)

# Define the file path and name for the CSV file
file_path <- "C:/Users/nicco/R/TRAC3D/THRESHING/5_variables_contributions.csv"

# Save the cross table as a CSV file
write.csv(coefficients_transposed, file = file_path, row.names = TRUE)

# Extract the coefficients for LD1 and LD2
coeff_LD1 <- coefficients["LD1", ]
coeff_LD2 <- coefficients["LD2", ]

# Sort the absolute values of the coefficients in descending order
sorted_LD1 <- sort(abs(coeff_LD1), decreasing = TRUE)
sorted_LD2 <- sort(abs(coeff_LD2), decreasing = TRUE)

# Extract the top 6 variables for LD1 and LD2
top_variables_LD1 <- names(sorted_LD1)[1:6]
top_variables_LD2 <- names(sorted_LD2)[1:6]

# Print the top variables for LD1 and LD2
cat("Top variables for LD1:\n")
print(top_variables_LD1)
cat("Top variables for LD2:\n")
print(top_variables_LD2)

# Select the unique set of variables that contribute the most to LD1 and LD2
selected_variables <- unique(c(top_variables_LD1, top_variables_LD2))
print(selected_variables)

#--------------------------------------------------CROSS TABLE WITH PREDICTION RESULTS 

# Perform prediction on the selected_training_data_clean
selected_training_data_clean$Prediction <- predict(cda_model, newdata = selected_training_data_clean)$class

# Create a cross table
cross_table <- table(selected_training_data_clean$CODE_COD, selected_training_data_clean$Prediction)

# Compute row percentages
row_percentages <- prop.table(cross_table, margin = 1) * 100

# Add row and column margins
cross_table_with_margins <- addmargins(cross_table, margin = 1:2)
row_percentages_with_margins <- addmargins(row_percentages, margin = 1)

# Combine counts and percentages
cross_table_combined <- cbind(cross_table_with_margins, row_percentages_with_margins)

# Rename the row and column names
rownames(cross_table_combined)[1:4] <- c("Grass_reeds", "Domesticated_cereals", "Threshing_sledges", "Sum")
colnames(cross_table_combined)[1:3] <- c("Grass_reeds", "Domesticated_cereals", "Threshing_sledges")
colnames(cross_table_combined)[4] <- "Sum"
colnames(cross_table_combined)[5:7] <- c("Grass_reeds", "Domesticated_cereals", "Threshing_sledges")

# Print the modified cross table
print(cross_table_combined)

# Get the counts of correct classifications for each class
correct_counts <- diag(cross_table_combined[1:3, 1:3])

# Calculate the total number of observations
total_observations <- sum(cross_table_combined[1:3, 1:3])

# Calculate the global percentage of cases correctly classified
global_percentage <- sum(correct_counts) / total_observations * 100

# Print the global percentage
cat("Global Percentage of Cases Correctly Classified:", global_percentage, "%\n")

# Define the file path and name for the Excel file
file_path <- "C:/Users/nicco/R/TRAC3D/THRESHING/7_results_training_all_var.csv"

# Save the cross table as a CSV file
write.csv(cross_table_combined, file = file_path, row.names = TRUE)


#--------------------------------------------------SCATTER PLOT

# Define the Darjeeling palette
darjeeling_palette <- c("#009E73", "#FF3300", "#56B4E9")

# Recode the Class variable using CODE_NAME
df <- df %>%
  mutate(Class = recode(Class,
                        "1" = "Grass_reeds",
                        "2" = "Domesticated_cereals",
                        "3" = "Threshing_sledges"))

# Create the scatter plot
scatter_plot_allvar <- ggplot(df, aes(x = LD1, y = LD2, color = Class)) +
  geom_point(shape = 21, size = 1.8, stroke = 0.5) +
  geom_point(data = centroids, aes(x = LD1, y = LD2), color = "#000000", size = 4, shape = 22, stroke = 0.5, fill = "#FFFF66") +
  labs(x = "LD1", y = "LD2", title = "Canonical Discriminant Classification") +
  scale_color_manual(name = "Class", values = darjeeling_palette) +
  theme(panel.background = element_rect(fill = "#FFFFFF")) +
  theme_minimal() +
  theme(panel.grid.major = element_line(color = "#999999", linetype = "dotted", size = 0.1),
        panel.grid.minor = element_line(color = "#999999", linetype = "dotted", size = 0.1))

# Define the file path and name for the JPG image
file_path <- "C:/Users/nicco/R/TRAC3D/THRESHING/8_scatter_plot.jpg"

# Save the scatter plot as a JPG image
ggsave(file = file_path, plot = scatter_plot_allvar, width = 10, height = 6, dpi = 600)

# Print the scatter plot
print(scatter_plot_allvar)


#--------------------------------------------------TEST MODEL ACCURACY

# Split the dataset into training and testing sets
set.seed(123)  # For reproducibility
index <- createDataPartition(selected_training_data_clean$CODE_COD, p = 0.7, list = FALSE)
training_data <- selected_training_data_clean[index, ]
testing_data <- selected_training_data_clean[-index, ]

# Train the multinomial logistic regression model
model <- multinom(CODE_COD ~ ., data = training_data)

# Predict on the testing set
predictions <- predict(model, newdata = testing_data, type = "class")

# Evaluate the predictions
confusion_matrix <- table(predictions, testing_data$CODE_COD)

# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", accuracy))

# Calculate precision, recall, and F1 score for each class
precision <- diag(confusion_matrix) / colSums(confusion_matrix)
recall <- diag(confusion_matrix) / rowSums(confusion_matrix)
f1_score <- 2 * ((precision * recall) / (precision + recall))

# Calculate macro-average and micro-average of precision, recall, and F1 score
macro_precision <- mean(precision)
macro_recall <- mean(recall)
macro_f1_score <- mean(f1_score)

micro_precision <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
micro_recall <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
micro_f1_score <- 2 * ((micro_precision * micro_recall) / (micro_precision + micro_recall))

# Print evaluation metrics
print("Evaluation Metrics:")
print(paste("Macro Precision:", macro_precision))
print(paste("Macro Recall:", macro_recall))
print(paste("Macro F1 Score:", macro_f1_score))
print(paste("Micro Precision:", micro_precision))
print(paste("Micro Recall:", micro_recall))
print(paste("Micro F1 Score:", micro_f1_score))

# Print the confusion matrix
print("Confusion Matrix:")
print(confusion_matrix)


#--------------------------------------------------APPLY AT THE TEST_DATA

# Select the unique set of variables that contribute the most to LD1 and LD2
selected_variables <- unique(c(top_variables_LD1, top_variables_LD2))
  
# Create a subset of the test data with the selected variables
selected_test_data <- test_data[, c(selected_variables)]

# Clean rows with missing values
selected_test_data <- selected_test_data[complete.cases(selected_test_data), ]
  
# Generate unique names for the variables
unique_names <- make.unique(colnames(selected_test_data))
  
# Rename the variables in the selected test data
colnames(selected_test_data) <- unique_names
  
# Perform blind classification using the trained model
test_predictions <- predict(cda_model, newdata = selected_test_data)
  
# Extract the discriminant scores for each observation
scores_blind <- as.data.frame(test_predictions$x)

# Add the predicted class labels to the selected test data
selected_test_data$Predicted_Class <- test_predictions$class
  
# Create a data frame with scores and class labels
df_blind <- data.frame(scores_blind, Predicted_Class = as.factor(selected_test_data$Predicted_Class))

# Recode the Predicted_Class variable using CODE_NAME
df_blind <- df_blind %>%
  mutate(Predicted_Class = recode(Predicted_Class,
                          "1" = "Grass_reeds",
                          "2" = "Domesticated_cereals",
                          "3" = "Threshing_sledges"))
  
# Define the Darjeeling palette
darjeeling_palette <- c("#009E73", "#FF3300", "#56B4E9")

# Plotting the known and unknown individuals together
  scatter_plot_test <- ggplot() +
    geom_point(data = df, aes(x = LD1, y = LD2, color = Class), shape = 21, size = 1.8, stroke = 0.5) +
    geom_point(data = df_blind, aes(x = LD1, y = LD2), shape = 5, size = 0.7, stroke = 0.2, color = "black") +
    labs(x = "LD1", y = "LD2", title = "Blind Classification") +
    scale_color_manual(name = "Predicted_Class", values = darjeeling_palette) +
    theme(panel.background = element_rect(fill = "#FFFFFF"),
    panel.grid.major = element_line(color = "#999999", linetype = "dotted", size = 0.4),
    panel.grid.minor = element_line(color = "#999999", linetype = "dotted", size = 0.4)) +
    theme_minimal()
  
# Define the file path and name for the JPG image
file_path <- "C:/Users/nicco/R/TRAC3D/THRESHING/12_scatter_plot_test.jpg"
  
# Print and save the scatter plot as a JPG image
print(scatter_plot_test)
ggsave(file = file_path, plot = scatter_plot_test, width = 10, height = 6, dpi = 600)
 
# Create a data frame with the TOOL, Predicted_Class, and ORIGINAL NAME columns
predictions <- data.frame(TOOL = test_data$TOOL, 
                          Predicted_Class = test_predictions$class, 
                          ORIGINAL_NAME = test_data$ORIGINAL_NAME)

# Define the file path and name for the CSV file
file_path <- "C:/Users/nicco/R/TRAC3D/THRESHING/13_predictions.csv"

# Save the predictions as a CSV file
write.csv(predictions, file = file_path, row.names = FALSE)
  
# Create a cross-tabulation of Predicted_Class and TOOL
cross_tab_pred <- table(predictions$Predicted_Class, predictions$TOOL)
  
# Calculate the percentage of individuals in each class for each TOOL
percentage <- prop.table(cross_tab_pred, margin = 2) * 100

# Combine counts and percentages
cross_tab_pred_combined <- cbind(cross_tab_pred, percentage)

# Rename the row and column names to match the previous table
rownames(cross_tab_pred_combined)[1:3] <- c("Grass_reeds", "Domesticated_cereals", "Threshing_sledges")
  
# Print the percentage table
print(cross_tab_pred_combined)
  
# Define the file path and name for the Excel file
file_path <- "C:/Users/nicco/R/TRAC3D/THRESHING/14_results.csv"
  
# Save the cross table as a CSV file
write.csv(cross_tab_pred_combined, file = file_path, row.names = TRUE)

# Calculate the percentage of individuals in Class 3
percentage_class3 <- percentage[3, ]
  
# Select the individuals with more than 50% for Class 3
selected_individuals <- percentage_class3[percentage_class3 > 60]
  
# Print the selected individuals
print(selected_individuals)
  
  
  
